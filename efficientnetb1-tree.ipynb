{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10828428,"sourceType":"datasetVersion","datasetId":6723755},{"sourceId":10861748,"sourceType":"datasetVersion","datasetId":6723640},{"sourceId":10866622,"sourceType":"datasetVersion","datasetId":6750782},{"sourceId":10866776,"sourceType":"datasetVersion","datasetId":6750878},{"sourceId":10871918,"sourceType":"datasetVersion","datasetId":6754651},{"sourceId":10881918,"sourceType":"datasetVersion","datasetId":6761683},{"sourceId":10881965,"sourceType":"datasetVersion","datasetId":6761722}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", tf.keras.mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) OPTIMIZED DATA LOADING USING `tf.data`\n# ====================================\ndef preprocess_image(image, label):\n    image = tf.image.resize(image, (224, 224))  # Resize to model input size\n    image = tf.keras.applications.efficientnet.preprocess_input(image)  # Normalize [-1, 1]\n    return image, label\n\ndef load_data(subset):\n    datagen = tf.keras.preprocessing.image_dataset_from_directory(\n        image_dir,\n        image_size=(224, 224),\n        batch_size=batch_size,\n        validation_split=0.2,\n        subset=subset,\n        seed=42,\n        label_mode='categorical'\n    )\n    return datagen.map(preprocess_image).cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n\ntrain_dataset = load_data(\"training\")\nval_dataset = load_data(\"validation\")\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"‚úÖ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"üöÄ Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\nüîÑ Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_dataset, validation_data=val_dataset, epochs=1)\n\n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"‚úÖ Model saved: {model_epoch_path}\")\n\n    print(\"‚úÖ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T15:34:39.559050Z","iopub.execute_input":"2025-02-26T15:34:39.559410Z","execution_failed":"2025-02-26T15:50:54.685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 64\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"‚úÖ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"üöÄ Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\nüîÑ Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1)\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"‚úÖ Model saved: {model_epoch_path}\")\n        \n    print(\"‚úÖ Training session complete!\")\n\n# , start_epoch=3\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# ====================================\n# 1) Basic Setup & Paths\n# ====================================\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"‚úÖ Mixed precision enabled\")\n\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nweights_path = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"  # Uploaded weights\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\n\nbatch_size = 16  # ‚úÖ Adjusted for better memory usage\ninput_shape = (224, 224, 3)\nepochs = 3\nlearning_rate = 0.0005\nfine_tune_layers = 50  # ‚úÖ Unfreeze last 50 layers\n\n# ====================================\n# 2) Detect Classes\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(f\"üìå Detected {num_classes} classes\")\n\n# ====================================\n# 3) Optimized Data Pipeline\n# ====================================\ndef preprocess_image(image, label):\n    image = tf.image.resize(image, (224, 224))\n    image = tf.keras.applications.efficientnet.preprocess_input(image)\n    return image, label\n\ndef load_data(subset):\n    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n        image_dir,\n        image_size=(224, 224),\n        batch_size=batch_size,\n        validation_split=0.2,\n        subset=subset,\n        seed=42,\n        label_mode='categorical'\n    )\n    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    if subset == \"training\":\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)  # ‚úÖ No `cache()` to save RAM\n    else:\n        dataset = dataset.cache().prefetch(tf.data.AUTOTUNE)  # ‚úÖ Cache only validation\n\n    return dataset\n\ntrain_dataset = load_data(\"training\")\nval_dataset = load_data(\"validation\")\n\n# ====================================\n# 4) Model Creation (Load Weights Manually)\n# ====================================\ndef create_model():\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=None,  # ‚úÖ Avoids Kaggle download restrictions\n        input_shape=input_shape\n    )\n\n    # ‚úÖ Load weights from uploaded file\n    if os.path.exists(weights_path):\n        base_model.load_weights(weights_path)\n        print(\"‚úÖ Loaded EfficientNetB3 pretrained weights\")\n    else:\n        print(\"‚ö†Ô∏è Warning: Pretrained weights not found! Using random initialization.\")\n\n    base_model.trainable = True\n    for layer in base_model.layers[:-fine_tune_layers]:  \n        layer.trainable = False  # ‚úÖ Freeze early layers\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.MaxPooling2D(pool_size=(2, 2)),  # ‚úÖ Saves memory\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.4),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\n# ====================================\n# 5) Load or Create Model\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"‚úÖ Loaded latest model\")\nelse:\n    model = create_model()\n    print(\"üöÄ Created new model with pretrained EfficientNetB3 weights\")\n\n# ====================================\n# 6) Training with Memory Fix\n# ====================================\nmodel.fit(train_dataset, validation_data=val_dataset, epochs=epochs)\nmodel.save(latest_model_path)\nprint(\"‚úÖ Model saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T03:35:27.178934Z","iopub.execute_input":"2025-02-27T03:35:27.179411Z","iopub.status.idle":"2025-02-27T03:43:02.076909Z","shell.execute_reply.started":"2025-02-27T03:35:27.179376Z","shell.execute_reply":"2025-02-27T03:43:02.075246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\ntf.config.optimizer.set_jit(True)  # Enable XLA compilation for faster training\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\n# Reduce batch size to prevent RAM issues\nbatch_size = 16  # Reduced from 32 to avoid out-of-memory errors\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS (OPTIMIZED)\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\nAUTOTUNE = tf.data.AUTOTUNE  # Optimize data loading\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys()),\n    workers=2,  # Use 2 workers for faster data loading\n    use_multiprocessing=True  # Enable multiprocessing\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys()),\n    workers=2,\n    use_multiprocessing=True\n)\n\n# Prefetch data for better performance\ntrain_dataset = tf.data.Dataset.from_generator(\n    lambda: train_generator,\n    output_signature=(\n        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32),\n    )\n).prefetch(buffer_size=AUTOTUNE)\n\nval_dataset = tf.data.Dataset.from_generator(\n    lambda: val_generator,\n    output_signature=(\n        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32),\n    )\n).prefetch(buffer_size=AUTOTUNE)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION (OPTIMIZED)\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 20  # Unfreeze last 20 layers (not 50)\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"‚úÖ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"üöÄ Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING (OPTIMIZED)\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\nüîÑ Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_dataset, validation_data=val_dataset, epochs=1)\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"‚úÖ Model saved: {model_epoch_path}\")\n        \n    print(\"‚úÖ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T03:43:05.849446Z","iopub.execute_input":"2025-02-27T03:43:05.849811Z","iopub.status.idle":"2025-02-27T03:43:05.915879Z","shell.execute_reply.started":"2025-02-27T03:43:05.849783Z","shell.execute_reply":"2025-02-27T03:43:05.914269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())  # Keep class order consistent\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"‚úÖ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"üöÄ Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\nüîÑ Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=1,\n            workers=2,  # Multi-threaded data loading\n            use_multiprocessing=True  # Reduce RAM usage\n        )\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"‚úÖ Model saved: {model_epoch_path}\")\n        \n    print(\"‚úÖ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T03:44:08.089818Z","iopub.execute_input":"2025-02-27T03:44:08.090221Z","iopub.status.idle":"2025-02-27T03:44:55.397692Z","shell.execute_reply.started":"2025-02-27T03:44:08.090192Z","shell.execute_reply":"2025-02-27T03:44:55.396231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())  # Keep class order consistent\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\nsteps_per_epoch = len(train_generator)  # Ensures controlled training on large datasets\nvalidation_steps = len(val_generator)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"‚úÖ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"üöÄ Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\nüîÑ Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=1,\n            steps_per_epoch=steps_per_epoch,\n            validation_steps=validation_steps\n        )\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"‚úÖ Model saved: {model_epoch_path}\")\n        \n    print(\"‚úÖ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T03:45:35.534110Z","iopub.execute_input":"2025-02-27T03:45:35.534634Z","iopub.status.idle":"2025-02-27T03:48:28.418237Z","shell.execute_reply.started":"2025-02-27T03:45:35.534598Z","shell.execute_reply":"2025-02-27T03:48:28.416630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.mixed_precision import set_global_policy\n\n# Enable mixed precision for GPU acceleration\ntry:\n    set_global_policy('mixed_float16')  # Remove this if using CPU\nexcept:\n    print(\"Mixed precision not available, using default float32.\")\n\n# Paths & parameters\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nbatch_size = 16  # Reduced to optimize RAM usage\ninput_shape = (300, 300, 3)  # Ensure resolution remains unchanged\nnum_classes = 27\n\n# Load EfficientNetB3 without top layers\nbase_model = EfficientNetB3(weights='/kaggle/input/efficientnetb3/efficientnetb3_notop.h5', \n                            include_top=False, input_shape=input_shape)\n\n# Freeze base model layers\nbase_model.trainable = False\n\n# Add custom classification head\nx = GlobalAveragePooling2D()(base_model.output)\nx = Dropout(0.4)(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.3)(x)\noutput = Dense(num_classes, activation='softmax', dtype='float32')(x)  # Ensure correct dtype\n\nmodel = Model(inputs=base_model.input, outputs=output)\n\n# Compile model\nmodel.compile(optimizer=Adam(learning_rate=0.0005),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Data augmentation & generators\ndatagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=False,  # Disable shuffle to reduce RAM usage\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n)\n\n# Training function to continue training across sessions\ndef train_model(model, epochs=2, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\nüîÑ Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=1,  # Train 1 epoch per run to prevent crashes\n            verbose=1\n        )\n        model.save(f\"/kaggle/working/tree_classifier_epoch_{epoch}.h5\")  # Save after each epoch\n\n# Start training\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Adjust this to fit within Kaggle limits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T03:48:50.789172Z","iopub.execute_input":"2025-02-27T03:48:50.789753Z","iopub.status.idle":"2025-02-27T03:50:38.797840Z","shell.execute_reply.started":"2025-02-27T03:48:50.789709Z","shell.execute_reply":"2025-02-27T03:50:38.796364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 16\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"‚úÖ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"üöÄ Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\nüîÑ Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1)\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"‚úÖ Model saved: {model_epoch_path}\")\n        \n    print(\"‚úÖ Training session complete!\")\n\n# , start_epoch=3\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T03:56:13.776450Z","iopub.execute_input":"2025-02-27T03:56:13.777014Z","iopub.status.idle":"2025-02-27T04:00:35.009860Z","shell.execute_reply.started":"2025-02-27T03:56:13.776973Z","shell.execute_reply":"2025-02-27T04:00:35.006333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\nimport matplotlib.pyplot as plt\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS (Kaggle)\n# ====================================\n# Update these paths according to your Kaggle environment:\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"  # Dataset folder; each subfolder is a class.\nsave_dir = \"/kaggle/working\"                        # Model, history, and logs will be saved here.\nos.makedirs(save_dir, exist_ok=True)\n\n# We will save individual epoch models with a prefix and also keep a \"latest\" copy.\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\n# IMPORTANT: Set the path to your local downloaded weights file.\n# Upload your EfficientNetB3 weights file (e.g. \"efficientnetb3_notop.h5\") to the Kaggle working directory.\nweights_file = \"//kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload your EfficientNetB3 weights file to: \" + weights_file)\nelse:\n    print(\"Using local weights file:\", weights_file)\n\n# Basic training parameters\nbatch_size = 32         # You can try reducing this (e.g., to 16) if you need faster epochs.\ninput_shape = (224, 224, 3)\nbase_learning_rate = 0.0005\ninitial_epochs = 3       # Number of epochs per training session in the initial phase\nfine_tune_epochs = 5     # Number of epochs per fine-tuning session\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\n# Automatically detect classes from subfolders\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir)\n    if os.path.isdir(os.path.join(image_dir, d))\n)\n\n# If a class indices JSON exists, update it; otherwise, create a new mapping.\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        saved_class_indices = json.load(f)\n    # Add any new classes\n    for new_class in detected_classes:\n        if new_class not in saved_class_indices:\n            saved_class_indices[new_class] = len(saved_class_indices)\n    class_indices = saved_class_indices\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Updated class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\n# Use EfficientNet's preprocess_input to normalize inputs to [-1, 1]\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n    rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())  # Fixes the class order\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    # Build the base using EfficientNetB3 with local weights\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,  # Use local weights\n        input_shape=input_shape\n    )\n    if fine_tune:\n        base_model.trainable = True\n        # Unfreeze the last 30 layers; keep earlier layers frozen\n        fine_tune_at = len(base_model.layers) - 30\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = 1e-5  # Lower learning rate for fine-tuning\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        # Explicitly output float32 for compatibility with mixed precision\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"‚úÖ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"üöÄ Created new model from scratch.\")\n\n# ====================================\n# 7) DEFINE CALLBACKS\n# ====================================\ncheckpoint_callback = ModelCheckpoint(\n    model_path,\n    save_weights_only=False,\n    save_best_only=False,  # Save every epoch\n    verbose=1\n)\nearly_stop = EarlyStopping(\n    patience=5,\n    restore_best_weights=True,\n    monitor='val_loss'\n)\nreduce_lr = ReduceLROnPlateau(\n    factor=0.5,\n    patience=2,\n    min_lr=1e-6,\n    verbose=1\n)\ntensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n\ncallbacks = [checkpoint_callback, early_stop, reduce_lr, tensorboard_callback]\n\n# ====================================\n# 8) TRAINING FUNCTION (WITH PER-EPOCH SAVING)\n# ====================================\ndef train_model(model, epochs=3, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\nüîÑ Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        # Train for one epoch at a time\n        model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=epoch + 1,\n            initial_epoch=epoch\n        )\n        # Save model after each epoch\n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"‚úÖ Model saved: {model_epoch_path}\")\n    print(\"‚úÖ Training session complete!\")\n\n# ====================================\n# 9) MAIN TRAINING LOOP (INITIAL PHASE)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\nwhile True:\n    ans = input(\"Run another initial training session? (yes/no): \")\n    if ans.strip().lower() == \"yes\":\n        train_model(model, epochs=3)  # Run 3 epochs per session\n    else:\n        break\n\n# ====================================\n# 10) FINE-TUNING PHASE (Optional)\n# ====================================\nans = input(\"Proceed to fine-tuning phase? (yes/no): \")\nif ans.strip().lower() == \"yes\":\n    ft_model = create_model(fine_tune=True)\n    ft_model.load_weights(latest_model_path)\n    print(\"üîÑ Starting fine-tuning with last 30 layers unfrozen.\")\n    train_model(ft_model, epochs=5)  # Fine-tune for 5 epochs per session\n    print(\"‚úÖ Fine-tuning complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T04:06:18.316607Z","iopub.execute_input":"2025-02-27T04:06:18.316988Z","iopub.status.idle":"2025-02-27T04:07:03.630575Z","shell.execute_reply.started":"2025-02-27T04:06:18.316956Z","shell.execute_reply":"2025-02-27T04:07:03.629187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# Enable Mixed Precision & XLA for Speed Boost üöÄ\nmixed_precision.set_global_policy('mixed_float16')\ntf.config.optimizer.set_jit(True)  # XLA Compilation\n\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 1) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\n# Hyperparameters (Optimized for Speed ‚ö°)\nbatch_size = 32  # Increased from 16 to 32\ninput_shape = (224, 224, 3)\ninitial_epochs = 2  # Reduce epochs per session\nfine_tune_epochs = 3  # Fine-tune with fewer epochs\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-5\n\n# ====================================\n# 2) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\n\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 3) DATA GENERATORS (Optimized)\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # EfficientNet built-in preprocessing\n    rotation_range=20,  # Reduced for speed\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 4) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.3),  # Reduced dropout for speed\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 5) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"‚úÖ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"üöÄ Created new model from scratch.\")\n\n# ====================================\n# 6) CALLBACKS FOR FASTER TRAINING\n# ====================================\ncheckpoint_callback = ModelCheckpoint(\n    latest_model_path,  # Save latest model\n    save_weights_only=False,\n    save_best_only=False,  # Save every epoch\n    verbose=1\n)\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=2,  # Stop early if no improvement\n    restore_best_weights=True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=1,\n    verbose=1\n)\n\ntensorboard_callback = TensorBoard(log_dir=log_dir)\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\nüîÑ Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=1,\n            callbacks=[checkpoint_callback, reduce_lr, tensorboard_callback]\n        )\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"‚úÖ Model saved: {model_epoch_path}\")\n        \n    print(\"‚úÖ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Reduced epochs to save time\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T04:10:52.674031Z","iopub.execute_input":"2025-02-27T04:10:52.674571Z","iopub.status.idle":"2025-02-27T04:14:06.850100Z","shell.execute_reply.started":"2025-02-27T04:10:52.674536Z","shell.execute_reply":"2025-02-27T04:14:06.848514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb2/efficientnetb2_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB2 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB2(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"‚úÖ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"üöÄ Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\nüîÑ Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1)\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"‚úÖ Model saved: {model_epoch_path}\")\n        \n    print(\"‚úÖ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T05:01:34.842595Z","iopub.execute_input":"2025-02-27T05:01:34.843194Z","execution_failed":"2025-02-27T05:09:00.687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"  # Update this if dataset path is different\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = \"/kaggle/input/latest-nineteen/latest_model1.keras\"\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\n# ‚úÖ Use locally uploaded EfficientNetB1 weights\nweights_file = \"/kaggle/input/efficientnetb1-notop/efficientnetb1_notop.h5\"  # Update the path if needed\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB1 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32  # Adjust based on memory availability\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes images to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB1(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"‚úÖ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"üöÄ Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\nüîÑ Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1)\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        # model.save(latest_model_path)\n        latest_model_path = os.path.join(save_dir, \"latest_model1.keras\")\n        model.save(latest_model_path)  # ‚úÖ Save in /kaggle/working/\n\n        print(f\"‚úÖ Model saved: {model_epoch_path}\")\n        \n    print(\"‚úÖ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=1, start_epoch=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T01:29:19.071891Z","iopub.execute_input":"2025-02-28T01:29:19.072265Z","execution_failed":"2025-02-28T04:28:19.956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nprint(\"Classes detected:\", sorted(os.listdir(image_dir)))\n!pip install tensorflow keras numpy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T04:31:01.566009Z","iopub.execute_input":"2025-02-28T04:31:01.566322Z","iopub.status.idle":"2025-02-28T04:31:07.339796Z","shell.execute_reply.started":"2025-02-28T04:31:01.566297Z","shell.execute_reply":"2025-02-28T04:31:07.338532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"  # Your dataset folder\nsave_dir = \"/kaggle/working\"  # Writable directory for saving checkpoints\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\n# For loading, use your uploaded model (in Kaggle input). Replace \"latest-nineteen\" with your actual folder.\nuploaded_model_path = \"/kaggle/input/latest-nineteenv3/latest_model1.keras\"\n# For saving, always use the working directory:\nlatest_model_path = os.path.join(save_dir, \"latest_model1.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\n# ‚úÖ Use locally uploaded EfficientNetB1 weights\nweights_file = \"/kaggle/input/efficientnetb1-notop/efficientnetb1_notop.h5\"  # Update path if needed\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB1 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32  # Adjust based on memory availability\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes images to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB1(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\n# Try to load the uploaded model first (from /kaggle/input)\nif os.path.exists(uploaded_model_path):\n    model = keras.models.load_model(uploaded_model_path)\n    print(\"‚úÖ Loaded latest model from uploaded file.\")\nelse:\n    model = create_model()\n    print(\"üöÄ Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model_func(model, epochs, start_epoch=1):\n    # Note: latest_model_path is defined above for saving in /kaggle/working/\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\nüîÑ Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1)\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)  # Save latest model in /kaggle/working/\n        print(f\"‚úÖ Model saved: {model_epoch_path}\")\n        \n    print(\"‚úÖ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING PHASE\n# ====================================\nprint(\"\\n=== Training Phase ===\")\n# For example, we resume training starting from epoch 2.\ntrain_model_func(model, epochs=3, start_epoch=4)\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# Before the session ends:\n#   1. Download the file at /kaggle/working/latest_model1.keras.\n# Next time:\n#   2. Upload the downloaded model to your Kaggle dataset (e.g., latest-nineteen).\n#   3. The above code will load the uploaded model from /kaggle/input/latest-nineteen/latest_model1.keras.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T16:31:12.307182Z","iopub.execute_input":"2025-02-28T16:31:12.307610Z","iopub.status.idle":"2025-02-28T18:54:41.305116Z","shell.execute_reply.started":"2025-02-28T16:31:12.307568Z","shell.execute_reply":"2025-02-28T18:54:41.303965Z"}},"outputs":[{"name":"stdout","text":"Mixed precision enabled: <DTypePolicy \"mixed_float16\">\nUsing local weights file: /kaggle/input/efficientnetb1-notop/efficientnetb1_notop.h5\nClass indices: {'Azadirachta_indica': 0, 'Baccharis_salicifolia': 1, 'Betula_pendula': 2, 'Camphora_officinarum': 3, 'Ceiba_pentandra': 4, 'Corylus_avellana': 5, 'Delonix_regia': 6, 'Eucalyptus_globulus': 7, 'Ficus_carica': 8, 'Ficus_microcarpa': 9, 'Juglans_regia': 10, 'Leucaena_leucocephala': 11, 'Ligustrum_lucidum': 12, 'Ligustrum_ovalifolium': 13, 'Malus_domestica': 14, 'Mangifera_indica': 15, 'Murraya_paniculata': 16, 'Olea_europaea': 17, 'Paulownia_tomentosa': 18, 'Pinus_halepensis': 19, 'Psidium_guajava': 20, 'Spathodea_campanulata': 21, 'Tecoma_stans': 22, 'Tilia_cordata': 23, 'Ulmus_parvifolia': 24, 'Washingtonia_robusta': 25, 'unknown': 26}\nFound 105631 images belonging to 27 classes.\nFound 26394 images belonging to 27 classes.\n‚úÖ Loaded latest model from uploaded file.\n\n=== Training Phase ===\n\nüîÑ Training epoch 4/6...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m3301/3301\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3349s\u001b[0m 1s/step - accuracy: 0.4826 - loss: 1.7367 - val_accuracy: 0.5587 - val_loss: 1.4807\n‚úÖ Model saved: /kaggle/working/model_epoch_4.keras\n\nüîÑ Training epoch 5/6...\n\u001b[1m3301/3301\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2483s\u001b[0m 751ms/step - accuracy: 0.4952 - loss: 1.6937 - val_accuracy: 0.5688 - val_loss: 1.4550\n‚úÖ Model saved: /kaggle/working/model_epoch_5.keras\n\nüîÑ Training epoch 6/6...\n\u001b[1m3301/3301\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2472s\u001b[0m 748ms/step - accuracy: 0.5042 - loss: 1.6667 - val_accuracy: 0.5722 - val_loss: 1.4381\n‚úÖ Model saved: /kaggle/working/model_epoch_6.keras\n‚úÖ Training session complete!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"  # Dataset folder\nsave_dir = \"/kaggle/working\"  # Save model & logs\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nuploaded_model_path = \"/kaggle/working/latest_model1.keras\"  # Latest model from previous training\nlatest_model_path = os.path.join(save_dir, \"latest_model1.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\n# ‚úÖ Use locally uploaded EfficientNetB1 weights\nweights_file = \"/kaggle/input/efficientnetb1-notop/efficientnetb1_notop.h5\"  # Ensure correct path\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB1 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32  # Can adjust based on GPU memory\ninput_shape = (224, 224, 3)\nfine_tune_epochs = 10  # Start with 10 epochs, can extend if needed\nfine_tune_learning_rate = 5e-6  # Lower LR to prevent overfitting\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes images to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE FINE-TUNING MODEL\n# ====================================\ndef create_model(fine_tune=True):\n    base_model = keras.applications.EfficientNetB1(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 100  # Unfreeze last 100 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD PREVIOUS MODEL & START FINE-TUNING\n# ====================================\nif os.path.exists(uploaded_model_path):\n    fine_tune_model = create_model(fine_tune=True)\n    fine_tune_model.load_weights(uploaded_model_path)\n    print(\"‚úÖ Loaded latest model from uploaded file. Starting fine-tuning...\")\nelse:\n    print(\"‚ùå No previous model found. Train the model first before fine-tuning!\")\n    exit()\n\n# ====================================\n# 7) TRAINING FUNCTION WITH CALLBACKS\n# ====================================\ndef train_model_func(model, epochs, start_epoch=1):\n    callbacks = [\n        ModelCheckpoint(latest_model_path, save_best_only=True, verbose=1),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n        TensorBoard(log_dir=log_dir)\n    ]\n\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\nüîÑ Fine-Tuning Epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1, callbacks=callbacks)\n\n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)  # Save latest model in /kaggle/working/\n        print(f\"‚úÖ Model saved: {model_epoch_path}\")\n\n    print(\"‚úÖ Fine-tuning complete!\")\n\n# ====================================\n# 8) START FINE-TUNING PHASE\n# ====================================\nprint(\"\\n=== Fine-Tuning Phase ===\")\ntrain_model_func(fine_tune_model, epochs=1, start_epoch=8)  # Start from epoch 7\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# Before the session ends:\n#   1. Download the file at /kaggle/working/latest_model1.keras.\n# Next time:\n#   2. Upload the downloaded model to your Kaggle dataset (e.g., latest-nineteen).\n#   3. The above code will load the uploaded model from /kaggle/input/latest-nineteen/latest_model1.keras.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T19:56:36.199483Z","iopub.execute_input":"2025-02-28T19:56:36.199834Z","execution_failed":"2025-02-28T20:01:30.710Z"}},"outputs":[{"name":"stdout","text":"Mixed precision enabled: <DTypePolicy \"mixed_float16\">\nUsing local weights file: /kaggle/input/efficientnetb1-notop/efficientnetb1_notop.h5\nClass indices: {'Azadirachta_indica': 0, 'Baccharis_salicifolia': 1, 'Betula_pendula': 2, 'Camphora_officinarum': 3, 'Ceiba_pentandra': 4, 'Corylus_avellana': 5, 'Delonix_regia': 6, 'Eucalyptus_globulus': 7, 'Ficus_carica': 8, 'Ficus_microcarpa': 9, 'Juglans_regia': 10, 'Leucaena_leucocephala': 11, 'Ligustrum_lucidum': 12, 'Ligustrum_ovalifolium': 13, 'Malus_domestica': 14, 'Mangifera_indica': 15, 'Murraya_paniculata': 16, 'Olea_europaea': 17, 'Paulownia_tomentosa': 18, 'Pinus_halepensis': 19, 'Psidium_guajava': 20, 'Spathodea_campanulata': 21, 'Tecoma_stans': 22, 'Tilia_cordata': 23, 'Ulmus_parvifolia': 24, 'Washingtonia_robusta': 25, 'unknown': 26}\nFound 105631 images belonging to 27 classes.\nFound 26394 images belonging to 27 classes.\n‚ùå No previous model found. Train the model first before fine-tuning!\n\n=== Fine-Tuning Phase ===\n\nüîÑ Fine-Tuning Epoch 8/8...\n\u001b[1m 368/3301\u001b[0m \u001b[32m‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m21:19\u001b[0m 436ms/step - accuracy: 0.4818 - loss: 1.7707","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Zip the model file (replace with your filename)\nshutil.make_archive('model_checkpoint', 'zip', '/kaggle/working/')\n\n# Now download\nfrom IPython.display import FileLink\nFileLink(r'model_checkpoint.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T13:48:13.933262Z","iopub.execute_input":"2025-02-28T13:48:13.933721Z","iopub.status.idle":"2025-02-28T13:50:22.184702Z","shell.execute_reply.started":"2025-02-28T13:48:13.933687Z","shell.execute_reply":"2025-02-28T13:50:22.182803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Create a ZIP file with only the model files\nshutil.make_archive('/kaggle/working/my_models', 'zip', '/kaggle/working', \n                     base_dir=None, verbose=True)\n\n# Generate a download link\nFileLink('/kaggle/working/my_models.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T13:56:24.819943Z","iopub.execute_input":"2025-02-28T13:56:24.820413Z","iopub.status.idle":"2025-02-28T13:58:31.156825Z","shell.execute_reply.started":"2025-02-28T13:56:24.820380Z","shell.execute_reply":"2025-02-28T13:58:31.155694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.copy('/kaggle/working/latest_model1.keras', '/kaggle/input/latest_model1.keras')\nshutil.copy('/kaggle/working/model_epoch_2.keras', '/kaggle/input/model_epoch_2.keras')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T13:58:56.329003Z","iopub.execute_input":"2025-02-28T13:58:56.329338Z","iopub.status.idle":"2025-02-28T13:58:56.356000Z","shell.execute_reply.started":"2025-02-28T13:58:56.329308Z","shell.execute_reply":"2025-02-28T13:58:56.354556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lh /kaggle/working/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:01:03.430561Z","iopub.execute_input":"2025-02-28T14:01:03.430947Z","iopub.status.idle":"2025-02-28T14:01:03.606134Z","shell.execute_reply.started":"2025-02-28T14:01:03.430920Z","shell.execute_reply":"2025-02-28T14:01:03.604880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nos.remove('/kaggle/working/model_checkpoint.zip')  # Delete the large ZIP\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:01:01.194529Z","iopub.execute_input":"2025-02-28T14:01:01.194959Z","iopub.status.idle":"2025-02-28T14:01:01.910809Z","shell.execute_reply.started":"2025-02-28T14:01:01.194926Z","shell.execute_reply":"2025-02-28T14:01:01.909715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\n\n# Define the zip file name\nzip_filename = \"/kaggle/working/my_models_fixed.zip\"\n\n# Create a zip file and add only the required models\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    zipf.write('/kaggle/working/latest_model1.keras', arcname='latest_model1.keras')\n    zipf.write('/kaggle/working/model_epoch_2.keras', arcname='model_epoch_2.keras')\n\n# Generate a download link\nfrom IPython.display import FileLink\nFileLink(zip_filename)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:01:19.243807Z","iopub.execute_input":"2025-02-28T14:01:19.244212Z","iopub.status.idle":"2025-02-28T14:01:19.568577Z","shell.execute_reply.started":"2025-02-28T14:01:19.244184Z","shell.execute_reply":"2025-02-28T14:01:19.567465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lh /kaggle/working/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:04:01.813241Z","iopub.execute_input":"2025-02-28T14:04:01.813822Z","iopub.status.idle":"2025-02-28T14:04:01.996429Z","shell.execute_reply.started":"2025-02-28T14:04:01.813782Z","shell.execute_reply":"2025-02-28T14:04:01.995020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:04:36.861846Z","iopub.execute_input":"2025-02-28T14:04:36.862274Z","iopub.status.idle":"2025-02-28T14:04:36.955910Z","shell.execute_reply.started":"2025-02-28T14:04:36.862242Z","shell.execute_reply":"2025-02-28T14:04:36.954290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\n\nzip_filename = \"/kaggle/working/my_models_fixed_nineteen.zip\"\n\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    zipf.write('/kaggle/working/latest_model1.keras', arcname='latest_model1.keras')\n    zipf.write('/kaggle/working/model_epoch_2.keras', arcname='model_epoch_2.keras')\n\nprint(\"‚úÖ Zip file created successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:05:16.345208Z","iopub.execute_input":"2025-02-28T14:05:16.345634Z","iopub.status.idle":"2025-02-28T14:05:16.716916Z","shell.execute_reply.started":"2025-02-28T14:05:16.345605Z","shell.execute_reply":"2025-02-28T14:05:16.715668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp /kaggle/working/my_models_fixed.zip /kaggle/working/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:05:54.151605Z","iopub.execute_input":"2025-02-28T14:05:54.152056Z","iopub.status.idle":"2025-02-28T14:05:54.364614Z","shell.execute_reply.started":"2025-02-28T14:05:54.152027Z","shell.execute_reply":"2025-02-28T14:05:54.363303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install kaggle\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:16:45.940517Z","iopub.execute_input":"2025-02-28T14:16:45.940962Z","iopub.status.idle":"2025-02-28T14:16:51.427534Z","shell.execute_reply.started":"2025-02-28T14:16:45.940928Z","shell.execute_reply":"2025-02-28T14:16:51.426168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p /kaggle/working/kaggle-dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:17:02.893466Z","iopub.execute_input":"2025-02-28T14:17:02.893845Z","iopub.status.idle":"2025-02-28T14:17:03.073142Z","shell.execute_reply.started":"2025-02-28T14:17:02.893817Z","shell.execute_reply":"2025-02-28T14:17:03.071785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"meta_data = '''{\n  \"title\": \"My Saved Models\",\n  \"id\": \"your-username/my-saved-models\",\n  \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}'''\n\nwith open(\"/kaggle/working/kaggle-dataset/dataset-metadata.json\", \"w\") as f:\n    f.write(meta_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:17:21.277765Z","iopub.execute_input":"2025-02-28T14:17:21.278162Z","iopub.status.idle":"2025-02-28T14:17:21.285595Z","shell.execute_reply.started":"2025-02-28T14:17:21.278132Z","shell.execute_reply":"2025-02-28T14:17:21.284574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mv /kaggle/working/*.keras /kaggle/working/kaggle-dataset/\n!mv /kaggle/working/*.zip /kaggle/working/kaggle-dataset/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:17:29.926665Z","iopub.execute_input":"2025-02-28T14:17:29.927058Z","iopub.status.idle":"2025-02-28T14:17:30.275914Z","shell.execute_reply.started":"2025-02-28T14:17:29.927028Z","shell.execute_reply":"2025-02-28T14:17:30.274239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset/ --public\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:18:35.442138Z","iopub.execute_input":"2025-02-28T14:18:35.442560Z","iopub.status.idle":"2025-02-28T14:18:36.301037Z","shell.execute_reply.started":"2025-02-28T14:18:35.442528Z","shell.execute_reply":"2025-02-28T14:18:36.299433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p /root/.kaggle\n!mv /kaggle/working/kaggle.json /root/.kaggle/\n!chmod 600 /root/.kaggle/kaggle.json\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:22:49.721189Z","iopub.execute_input":"2025-02-28T14:22:49.721701Z","iopub.status.idle":"2025-02-28T14:22:50.242676Z","shell.execute_reply.started":"2025-02-28T14:22:49.721661Z","shell.execute_reply":"2025-02-28T14:22:50.240929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p /root/.kaggle\n!cp \"/kaggle/input/kaggle/kaggle (1).json\" /root/.kaggle/kaggle.json\n!chmod 600 /root/.kaggle/kaggle.json\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:25:04.473260Z","iopub.execute_input":"2025-02-28T14:25:04.473713Z","iopub.status.idle":"2025-02-28T14:25:04.991208Z","shell.execute_reply.started":"2025-02-28T14:25:04.473678Z","shell.execute_reply":"2025-02-28T14:25:04.989667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets list\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:25:15.436653Z","iopub.execute_input":"2025-02-28T14:25:15.437041Z","iopub.status.idle":"2025-02-28T14:26:28.887924Z","shell.execute_reply.started":"2025-02-28T14:25:15.437013Z","shell.execute_reply":"2025-02-28T14:26:28.886463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset/ --public\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:26:30.136720Z","iopub.execute_input":"2025-02-28T14:26:30.137134Z","iopub.status.idle":"2025-02-28T14:27:19.793047Z","shell.execute_reply.started":"2025-02-28T14:26:30.137100Z","shell.execute_reply":"2025-02-28T14:27:19.791619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset/ --private\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:31:20.737236Z","iopub.execute_input":"2025-02-28T14:31:20.737764Z","iopub.status.idle":"2025-02-28T14:31:21.337260Z","shell.execute_reply.started":"2025-02-28T14:31:20.737720Z","shell.execute_reply":"2025-02-28T14:31:21.335974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/my_models.zip /kaggle/working/kaggle-dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:31:59.196007Z","iopub.execute_input":"2025-02-28T14:31:59.196390Z","iopub.status.idle":"2025-02-28T14:32:06.073453Z","shell.execute_reply.started":"2025-02-28T14:31:59.196342Z","shell.execute_reply":"2025-02-28T14:32:06.072116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset --public\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:57:01.422292Z","iopub.execute_input":"2025-02-28T14:57:01.422742Z","iopub.status.idle":"2025-02-28T14:57:08.132288Z","shell.execute_reply.started":"2025-02-28T14:57:01.422708Z","shell.execute_reply":"2025-02-28T14:57:08.130795Z"}},"outputs":[{"name":"stdout","text":"Starting upload for file latest_model1.keras\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39.0M/39.0M [00:01<00:00, 35.1MB/s]\nUpload successful: latest_model1.keras (39MB)\nStarting upload for file my_models_fixed_nineteen.zip\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78.0M/78.0M [00:01<00:00, 54.8MB/s]\nUpload successful: my_models_fixed_nineteen.zip (78MB)\nStarting upload for file model_epoch_2.keras\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39.0M/39.0M [00:01<00:00, 37.5MB/s]\nUpload successful: model_epoch_2.keras (39MB)\nStarting upload for file my_models_fixed.zip\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78.0M/78.0M [00:01<00:00, 53.0MB/s]\nUpload successful: my_models_fixed.zip (78MB)\nDataset creation error: Invalid Owner Id\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"!mkdir -p ~/.kaggle\n!cp /kaggle/input/kaggle/kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T16:25:23.065634Z","iopub.execute_input":"2025-02-28T16:25:23.065963Z","iopub.status.idle":"2025-02-28T16:25:23.628392Z","shell.execute_reply.started":"2025-02-28T16:25:23.065936Z","shell.execute_reply":"2025-02-28T16:25:23.627203Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset --public\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:01:31.765433Z","iopub.execute_input":"2025-02-28T15:01:31.765842Z","iopub.status.idle":"2025-02-28T15:01:38.973796Z","shell.execute_reply.started":"2025-02-28T15:01:31.765812Z","shell.execute_reply":"2025-02-28T15:01:38.972399Z"}},"outputs":[{"name":"stdout","text":"Starting upload for file latest_model1.keras\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39.0M/39.0M [00:01<00:00, 35.6MB/s]\nUpload successful: latest_model1.keras (39MB)\nStarting upload for file my_models_fixed_nineteen.zip\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78.0M/78.0M [00:01<00:00, 48.9MB/s]\nUpload successful: my_models_fixed_nineteen.zip (78MB)\nStarting upload for file model_epoch_2.keras\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39.0M/39.0M [00:01<00:00, 37.7MB/s]\nUpload successful: model_epoch_2.keras (39MB)\nStarting upload for file my_models_fixed.zip\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78.0M/78.0M [00:01<00:00, 50.2MB/s]\nUpload successful: my_models_fixed.zip (78MB)\nDataset creation error: Invalid Owner Id\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"!kaggle config view\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:02:20.494304Z","iopub.execute_input":"2025-02-28T15:02:20.494712Z","iopub.status.idle":"2025-02-28T15:02:21.080794Z","shell.execute_reply.started":"2025-02-28T15:02:20.494682Z","shell.execute_reply":"2025-02-28T15:02:21.079251Z"}},"outputs":[{"name":"stdout","text":"Configuration values from /root/.kaggle\n- username: mohammedelhajsayed\n- path: None\n- proxy: None\n- competition: None\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset --user mohammedelhajsayed --public\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:02:44.501560Z","iopub.execute_input":"2025-02-28T15:02:44.501927Z","iopub.status.idle":"2025-02-28T15:02:45.081080Z","shell.execute_reply.started":"2025-02-28T15:02:44.501900Z","shell.execute_reply":"2025-02-28T15:02:45.079632Z"}},"outputs":[{"name":"stdout","text":"usage: kaggle [-h] [-v] [-W] {competitions,c,datasets,d,kernels,k,models,m,files,f,config} ...\nkaggle: error: unrecognized arguments: --user mohammedelhajsayed\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"!ls /kaggle/working/kaggle-dataset/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:03:39.318433Z","iopub.execute_input":"2025-02-28T15:03:39.318972Z","iopub.status.idle":"2025-02-28T15:03:39.514403Z","shell.execute_reply.started":"2025-02-28T15:03:39.318935Z","shell.execute_reply":"2025-02-28T15:03:39.513036Z"}},"outputs":[{"name":"stdout","text":"dataset-metadata.json  model_epoch_2.keras\t     my_models_fixed.zip\nlatest_model1.keras    my_models_fixed_nineteen.zip\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset --public\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:04:06.151157Z","iopub.execute_input":"2025-02-28T15:04:06.151580Z","iopub.status.idle":"2025-02-28T15:04:12.924188Z","shell.execute_reply.started":"2025-02-28T15:04:06.151547Z","shell.execute_reply":"2025-02-28T15:04:12.922793Z"}},"outputs":[{"name":"stdout","text":"Starting upload for file latest_model1.keras\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39.0M/39.0M [00:01<00:00, 36.4MB/s]\nUpload successful: latest_model1.keras (39MB)\nStarting upload for file my_models_fixed_nineteen.zip\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78.0M/78.0M [00:01<00:00, 55.3MB/s]\nUpload successful: my_models_fixed_nineteen.zip (78MB)\nStarting upload for file model_epoch_2.keras\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39.0M/39.0M [00:01<00:00, 39.6MB/s]\nUpload successful: model_epoch_2.keras (39MB)\nStarting upload for file my_models_fixed.zip\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78.0M/78.0M [00:01<00:00, 46.4MB/s]\nUpload successful: my_models_fixed.zip (78MB)\nDataset creation error: Invalid Owner Id\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"!cat /kaggle/working/kaggle-dataset/dataset-metadata.json\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T15:04:19.523904Z","iopub.execute_input":"2025-02-28T15:04:19.524402Z","iopub.status.idle":"2025-02-28T15:04:19.703228Z","shell.execute_reply.started":"2025-02-28T15:04:19.524310Z","shell.execute_reply":"2025-02-28T15:04:19.701502Z"}},"outputs":[{"name":"stdout","text":"{\n  \"title\": \"My Saved Models\",\n  \"id\": \"your-username/my-saved-models\",\n  \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"%%writefile /kaggle/working/kaggle-dataset/dataset-metadata.json\n{\n    \"title\": \"MyModelCheckpoints\",\n    \"id\": \"mohammedelhajsayed/mymodelcheckpoints\",\n    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T19:55:06.487250Z","iopub.execute_input":"2025-02-28T19:55:06.487577Z","iopub.status.idle":"2025-02-28T19:55:06.494024Z","shell.execute_reply.started":"2025-02-28T19:55:06.487547Z","shell.execute_reply":"2025-02-28T19:55:06.493142Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/kaggle-dataset/dataset-metadata.json\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset --public\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T16:24:49.241875Z","iopub.execute_input":"2025-02-28T16:24:49.242221Z","iopub.status.idle":"2025-02-28T16:24:49.932150Z","shell.execute_reply.started":"2025-02-28T16:24:49.242189Z","shell.execute_reply":"2025-02-28T16:24:49.931081Z"}},"outputs":[{"name":"stdout","text":"Traceback (most recent call last):\n  File \"/usr/local/bin/kaggle\", line 5, in <module>\n    from kaggle.cli import main\n  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 7, in <module>\n    api.authenticate()\n  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 407, in authenticate\n    raise IOError('Could not find {}. Make sure it\\'s located in'\nOSError: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p /kaggle/working/kaggle-dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T19:53:51.602254Z","iopub.execute_input":"2025-02-28T19:53:51.603016Z","iopub.status.idle":"2025-02-28T19:53:51.839988Z","shell.execute_reply.started":"2025-02-28T19:53:51.602982Z","shell.execute_reply":"2025-02-28T19:53:51.839052Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import shutil\n\nshutil.move(\"/kaggle/working/latest_model1.keras\", \"/kaggle/working/kaggle-dataset/latest_model1.keras\")\nshutil.move(\"/kaggle/working/model_epoch_7.keras\", \"/kaggle/working/kaggle-dataset/model_epoch_7.keras\")\n# shutil.move(\"/kaggle/working/model_epoch_5.keras\", \"/kaggle/working/kaggle-dataset/model_epoch_5.keras\")\n# shutil.move(\"/kaggle/working/model_epoch_6.keras\", \"/kaggle/working/kaggle-dataset/model_epoch_6.keras\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T19:54:08.660753Z","iopub.execute_input":"2025-02-28T19:54:08.661140Z","iopub.status.idle":"2025-02-28T19:54:08.669929Z","shell.execute_reply.started":"2025-02-28T19:54:08.661111Z","shell.execute_reply":"2025-02-28T19:54:08.668985Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/kaggle-dataset/model_epoch_7.keras'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"metadata = '''{\n  \"title\": \"My Model Checkpoints\",\n  \"id\": \"your-kaggle-username/my-model-checkpoints\",\n  \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}'''\n\nwith open(\"/kaggle/working/kaggle-dataset/dataset-metadata.json\", \"w\") as f:\n    f.write(metadata)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T19:54:11.106942Z","iopub.execute_input":"2025-02-28T19:54:11.107242Z","iopub.status.idle":"2025-02-28T19:54:11.112029Z","shell.execute_reply.started":"2025-02-28T19:54:11.107217Z","shell.execute_reply":"2025-02-28T19:54:11.111294Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!mkdir -p /root/.kaggle\n!cp /kaggle/input/kaggle/kaggle.json /root/.kaggle/kaggle.json\n!chmod 600 /root/.kaggle/kaggle.json\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T19:54:12.974919Z","iopub.execute_input":"2025-02-28T19:54:12.975184Z","iopub.status.idle":"2025-02-28T19:54:13.655122Z","shell.execute_reply.started":"2025-02-28T19:54:12.975162Z","shell.execute_reply":"2025-02-28T19:54:13.654017Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset --public\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T19:55:15.197476Z","iopub.execute_input":"2025-02-28T19:55:15.197801Z","iopub.status.idle":"2025-02-28T19:55:20.355079Z","shell.execute_reply.started":"2025-02-28T19:55:15.197777Z","shell.execute_reply":"2025-02-28T19:55:20.354048Z"}},"outputs":[{"name":"stdout","text":"Starting upload for file model_epoch_7.keras\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 119M/119M [00:01<00:00, 108MB/s]\nUpload successful: model_epoch_7.keras (119MB)\nStarting upload for file latest_model1.keras\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 119M/119M [00:01<00:00, 110MB/s]\nUpload successful: latest_model1.keras (119MB)\nYour public Dataset is being created. Please check progress at https://www.kaggle.com/datasets/mohammedelhajsayed/mymodelcheckpoints\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}