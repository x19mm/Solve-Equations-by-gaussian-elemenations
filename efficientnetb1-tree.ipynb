{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10828428,"sourceType":"datasetVersion","datasetId":6723755},{"sourceId":10861748,"sourceType":"datasetVersion","datasetId":6723640},{"sourceId":10866622,"sourceType":"datasetVersion","datasetId":6750782},{"sourceId":10866776,"sourceType":"datasetVersion","datasetId":6750878},{"sourceId":10871918,"sourceType":"datasetVersion","datasetId":6754651},{"sourceId":10881918,"sourceType":"datasetVersion","datasetId":6761683},{"sourceId":10881965,"sourceType":"datasetVersion","datasetId":6761722},{"sourceId":10882575,"sourceType":"datasetVersion","datasetId":6762185},{"sourceId":10883578,"sourceType":"datasetVersion","datasetId":6762943},{"sourceId":10884841,"sourceType":"datasetVersion","datasetId":6763820},{"sourceId":10885291,"sourceType":"datasetVersion","datasetId":6764133},{"sourceId":10885989,"sourceType":"datasetVersion","datasetId":6764533},{"sourceId":10886718,"sourceType":"datasetVersion","datasetId":6764957},{"sourceId":10887421,"sourceType":"datasetVersion","datasetId":6765392},{"sourceId":10888187,"sourceType":"datasetVersion","datasetId":6765870},{"sourceId":10888824,"sourceType":"datasetVersion","datasetId":6766352},{"sourceId":10889500,"sourceType":"datasetVersion","datasetId":6766869}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", tf.keras.mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) OPTIMIZED DATA LOADING USING `tf.data`\n# ====================================\ndef preprocess_image(image, label):\n    image = tf.image.resize(image, (224, 224))  # Resize to model input size\n    image = tf.keras.applications.efficientnet.preprocess_input(image)  # Normalize [-1, 1]\n    return image, label\n\ndef load_data(subset):\n    datagen = tf.keras.preprocessing.image_dataset_from_directory(\n        image_dir,\n        image_size=(224, 224),\n        batch_size=batch_size,\n        validation_split=0.2,\n        subset=subset,\n        seed=42,\n        label_mode='categorical'\n    )\n    return datagen.map(preprocess_image).cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n\ntrain_dataset = load_data(\"training\")\nval_dataset = load_data(\"validation\")\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"✅ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"🚀 Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_dataset, validation_data=val_dataset, epochs=1)\n\n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"✅ Model saved: {model_epoch_path}\")\n\n    print(\"✅ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 64\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"✅ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"🚀 Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1)\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"✅ Model saved: {model_epoch_path}\")\n        \n    print(\"✅ Training session complete!\")\n\n# , start_epoch=3\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# ====================================\n# 1) Basic Setup & Paths\n# ====================================\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nprint(\"✅ Mixed precision enabled\")\n\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nweights_path = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"  # Uploaded weights\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\n\nbatch_size = 16  # ✅ Adjusted for better memory usage\ninput_shape = (224, 224, 3)\nepochs = 3\nlearning_rate = 0.0005\nfine_tune_layers = 50  # ✅ Unfreeze last 50 layers\n\n# ====================================\n# 2) Detect Classes\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(f\"📌 Detected {num_classes} classes\")\n\n# ====================================\n# 3) Optimized Data Pipeline\n# ====================================\ndef preprocess_image(image, label):\n    image = tf.image.resize(image, (224, 224))\n    image = tf.keras.applications.efficientnet.preprocess_input(image)\n    return image, label\n\ndef load_data(subset):\n    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n        image_dir,\n        image_size=(224, 224),\n        batch_size=batch_size,\n        validation_split=0.2,\n        subset=subset,\n        seed=42,\n        label_mode='categorical'\n    )\n    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    if subset == \"training\":\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)  # ✅ No `cache()` to save RAM\n    else:\n        dataset = dataset.cache().prefetch(tf.data.AUTOTUNE)  # ✅ Cache only validation\n\n    return dataset\n\ntrain_dataset = load_data(\"training\")\nval_dataset = load_data(\"validation\")\n\n# ====================================\n# 4) Model Creation (Load Weights Manually)\n# ====================================\ndef create_model():\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=None,  # ✅ Avoids Kaggle download restrictions\n        input_shape=input_shape\n    )\n\n    # ✅ Load weights from uploaded file\n    if os.path.exists(weights_path):\n        base_model.load_weights(weights_path)\n        print(\"✅ Loaded EfficientNetB3 pretrained weights\")\n    else:\n        print(\"⚠️ Warning: Pretrained weights not found! Using random initialization.\")\n\n    base_model.trainable = True\n    for layer in base_model.layers[:-fine_tune_layers]:  \n        layer.trainable = False  # ✅ Freeze early layers\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.MaxPooling2D(pool_size=(2, 2)),  # ✅ Saves memory\n        layers.Flatten(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.4),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\n# ====================================\n# 5) Load or Create Model\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"✅ Loaded latest model\")\nelse:\n    model = create_model()\n    print(\"🚀 Created new model with pretrained EfficientNetB3 weights\")\n\n# ====================================\n# 6) Training with Memory Fix\n# ====================================\nmodel.fit(train_dataset, validation_data=val_dataset, epochs=epochs)\nmodel.save(latest_model_path)\nprint(\"✅ Model saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\ntf.config.optimizer.set_jit(True)  # Enable XLA compilation for faster training\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\n# Reduce batch size to prevent RAM issues\nbatch_size = 16  # Reduced from 32 to avoid out-of-memory errors\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS (OPTIMIZED)\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\nAUTOTUNE = tf.data.AUTOTUNE  # Optimize data loading\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys()),\n    workers=2,  # Use 2 workers for faster data loading\n    use_multiprocessing=True  # Enable multiprocessing\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys()),\n    workers=2,\n    use_multiprocessing=True\n)\n\n# Prefetch data for better performance\ntrain_dataset = tf.data.Dataset.from_generator(\n    lambda: train_generator,\n    output_signature=(\n        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32),\n    )\n).prefetch(buffer_size=AUTOTUNE)\n\nval_dataset = tf.data.Dataset.from_generator(\n    lambda: val_generator,\n    output_signature=(\n        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32),\n    )\n).prefetch(buffer_size=AUTOTUNE)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION (OPTIMIZED)\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 20  # Unfreeze last 20 layers (not 50)\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"✅ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"🚀 Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING (OPTIMIZED)\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_dataset, validation_data=val_dataset, epochs=1)\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"✅ Model saved: {model_epoch_path}\")\n        \n    print(\"✅ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())  # Keep class order consistent\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"✅ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"🚀 Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=1,\n            workers=2,  # Multi-threaded data loading\n            use_multiprocessing=True  # Reduce RAM usage\n        )\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"✅ Model saved: {model_epoch_path}\")\n        \n    print(\"✅ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())  # Keep class order consistent\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\nsteps_per_epoch = len(train_generator)  # Ensures controlled training on large datasets\nvalidation_steps = len(val_generator)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"✅ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"🚀 Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=1,\n            steps_per_epoch=steps_per_epoch,\n            validation_steps=validation_steps\n        )\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"✅ Model saved: {model_epoch_path}\")\n        \n    print(\"✅ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.mixed_precision import set_global_policy\n\n# Enable mixed precision for GPU acceleration\ntry:\n    set_global_policy('mixed_float16')  # Remove this if using CPU\nexcept:\n    print(\"Mixed precision not available, using default float32.\")\n\n# Paths & parameters\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nbatch_size = 16  # Reduced to optimize RAM usage\ninput_shape = (300, 300, 3)  # Ensure resolution remains unchanged\nnum_classes = 27\n\n# Load EfficientNetB3 without top layers\nbase_model = EfficientNetB3(weights='/kaggle/input/efficientnetb3/efficientnetb3_notop.h5', \n                            include_top=False, input_shape=input_shape)\n\n# Freeze base model layers\nbase_model.trainable = False\n\n# Add custom classification head\nx = GlobalAveragePooling2D()(base_model.output)\nx = Dropout(0.4)(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.3)(x)\noutput = Dense(num_classes, activation='softmax', dtype='float32')(x)  # Ensure correct dtype\n\nmodel = Model(inputs=base_model.input, outputs=output)\n\n# Compile model\nmodel.compile(optimizer=Adam(learning_rate=0.0005),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Data augmentation & generators\ndatagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=False,  # Disable shuffle to reduce RAM usage\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n)\n\n# Training function to continue training across sessions\ndef train_model(model, epochs=2, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=1,  # Train 1 epoch per run to prevent crashes\n            verbose=1\n        )\n        model.save(f\"/kaggle/working/tree_classifier_epoch_{epoch}.h5\")  # Save after each epoch\n\n# Start training\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Adjust this to fit within Kaggle limits\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 16\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"✅ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"🚀 Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1)\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"✅ Model saved: {model_epoch_path}\")\n        \n    print(\"✅ Training session complete!\")\n\n# , start_epoch=3\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\nimport matplotlib.pyplot as plt\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS (Kaggle)\n# ====================================\n# Update these paths according to your Kaggle environment:\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"  # Dataset folder; each subfolder is a class.\nsave_dir = \"/kaggle/working\"                        # Model, history, and logs will be saved here.\nos.makedirs(save_dir, exist_ok=True)\n\n# We will save individual epoch models with a prefix and also keep a \"latest\" copy.\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\n# IMPORTANT: Set the path to your local downloaded weights file.\n# Upload your EfficientNetB3 weights file (e.g. \"efficientnetb3_notop.h5\") to the Kaggle working directory.\nweights_file = \"//kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload your EfficientNetB3 weights file to: \" + weights_file)\nelse:\n    print(\"Using local weights file:\", weights_file)\n\n# Basic training parameters\nbatch_size = 32         # You can try reducing this (e.g., to 16) if you need faster epochs.\ninput_shape = (224, 224, 3)\nbase_learning_rate = 0.0005\ninitial_epochs = 3       # Number of epochs per training session in the initial phase\nfine_tune_epochs = 5     # Number of epochs per fine-tuning session\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\n# Automatically detect classes from subfolders\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir)\n    if os.path.isdir(os.path.join(image_dir, d))\n)\n\n# If a class indices JSON exists, update it; otherwise, create a new mapping.\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        saved_class_indices = json.load(f)\n    # Add any new classes\n    for new_class in detected_classes:\n        if new_class not in saved_class_indices:\n            saved_class_indices[new_class] = len(saved_class_indices)\n    class_indices = saved_class_indices\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Updated class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\n# Use EfficientNet's preprocess_input to normalize inputs to [-1, 1]\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n    rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())  # Fixes the class order\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    # Build the base using EfficientNetB3 with local weights\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,  # Use local weights\n        input_shape=input_shape\n    )\n    if fine_tune:\n        base_model.trainable = True\n        # Unfreeze the last 30 layers; keep earlier layers frozen\n        fine_tune_at = len(base_model.layers) - 30\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = 1e-5  # Lower learning rate for fine-tuning\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        # Explicitly output float32 for compatibility with mixed precision\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"✅ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"🚀 Created new model from scratch.\")\n\n# ====================================\n# 7) DEFINE CALLBACKS\n# ====================================\ncheckpoint_callback = ModelCheckpoint(\n    model_path,\n    save_weights_only=False,\n    save_best_only=False,  # Save every epoch\n    verbose=1\n)\nearly_stop = EarlyStopping(\n    patience=5,\n    restore_best_weights=True,\n    monitor='val_loss'\n)\nreduce_lr = ReduceLROnPlateau(\n    factor=0.5,\n    patience=2,\n    min_lr=1e-6,\n    verbose=1\n)\ntensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n\ncallbacks = [checkpoint_callback, early_stop, reduce_lr, tensorboard_callback]\n\n# ====================================\n# 8) TRAINING FUNCTION (WITH PER-EPOCH SAVING)\n# ====================================\ndef train_model(model, epochs=3, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        # Train for one epoch at a time\n        model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=epoch + 1,\n            initial_epoch=epoch\n        )\n        # Save model after each epoch\n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"✅ Model saved: {model_epoch_path}\")\n    print(\"✅ Training session complete!\")\n\n# ====================================\n# 9) MAIN TRAINING LOOP (INITIAL PHASE)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\nwhile True:\n    ans = input(\"Run another initial training session? (yes/no): \")\n    if ans.strip().lower() == \"yes\":\n        train_model(model, epochs=3)  # Run 3 epochs per session\n    else:\n        break\n\n# ====================================\n# 10) FINE-TUNING PHASE (Optional)\n# ====================================\nans = input(\"Proceed to fine-tuning phase? (yes/no): \")\nif ans.strip().lower() == \"yes\":\n    ft_model = create_model(fine_tune=True)\n    ft_model.load_weights(latest_model_path)\n    print(\"🔄 Starting fine-tuning with last 30 layers unfrozen.\")\n    train_model(ft_model, epochs=5)  # Fine-tune for 5 epochs per session\n    print(\"✅ Fine-tuning complete!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# Enable Mixed Precision & XLA for Speed Boost 🚀\nmixed_precision.set_global_policy('mixed_float16')\ntf.config.optimizer.set_jit(True)  # XLA Compilation\n\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 1) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\n# Hyperparameters (Optimized for Speed ⚡)\nbatch_size = 32  # Increased from 16 to 32\ninput_shape = (224, 224, 3)\ninitial_epochs = 2  # Reduce epochs per session\nfine_tune_epochs = 3  # Fine-tune with fewer epochs\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-5\n\n# ====================================\n# 2) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\n\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 3) DATA GENERATORS (Optimized)\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # EfficientNet built-in preprocessing\n    rotation_range=20,  # Reduced for speed\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 4) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.3),  # Reduced dropout for speed\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 5) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"✅ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"🚀 Created new model from scratch.\")\n\n# ====================================\n# 6) CALLBACKS FOR FASTER TRAINING\n# ====================================\ncheckpoint_callback = ModelCheckpoint(\n    latest_model_path,  # Save latest model\n    save_weights_only=False,\n    save_best_only=False,  # Save every epoch\n    verbose=1\n)\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=2,  # Stop early if no improvement\n    restore_best_weights=True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=1,\n    verbose=1\n)\n\ntensorboard_callback = TensorBoard(log_dir=log_dir)\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=1,\n            callbacks=[checkpoint_callback, reduce_lr, tensorboard_callback]\n        )\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"✅ Model saved: {model_epoch_path}\")\n        \n    print(\"✅ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Reduced epochs to save time\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = os.path.join(save_dir, \"latest_model.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nweights_file = \"/kaggle/input/efficientnetb2/efficientnetb2_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB2 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB2(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"✅ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"🚀 Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1)\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)\n        print(f\"✅ Model saved: {model_epoch_path}\")\n        \n    print(\"✅ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=2)  # Run only 2 epochs per Kaggle session\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"  # Update this if dataset path is different\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nlatest_model_path = \"/kaggle/input/latest-nineteen/latest_model1.keras\"\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\n# ✅ Use locally uploaded EfficientNetB1 weights\nweights_file = \"/kaggle/input/efficientnetb1-notop/efficientnetb1_notop.h5\"  # Update the path if needed\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB1 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32  # Adjust based on memory availability\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes images to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB1(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\nif os.path.exists(latest_model_path):\n    model = keras.models.load_model(latest_model_path)\n    print(\"✅ Loaded latest model from disk.\")\nelse:\n    model = create_model()\n    print(\"🚀 Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model(model, epochs, start_epoch=1):\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1)\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        # model.save(latest_model_path)\n        latest_model_path = os.path.join(save_dir, \"latest_model1.keras\")\n        model.save(latest_model_path)  # ✅ Save in /kaggle/working/\n\n        print(f\"✅ Model saved: {model_epoch_path}\")\n        \n    print(\"✅ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING (SAVES EVERY EPOCH)\n# ====================================\nprint(\"\\n=== Initial Training Phase ===\")\ntrain_model(model, epochs=1, start_epoch=2)  # Run only 2 epochs per Kaggle session\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# 1. Upload the latest saved model (e.g., model_epoch_2.keras) to Kaggle input.\n# 2. Update 'latest_model_path' to point to the uploaded model.\n# 3. Restart training from the next epoch.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nprint(\"Classes detected:\", sorted(os.listdir(image_dir)))\n!pip install tensorflow keras numpy\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"  # Your dataset folder\nsave_dir = \"/kaggle/working\"  # Writable directory for saving checkpoints\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\n# For loading, use your uploaded model (in Kaggle input). Replace \"latest-nineteen\" with your actual folder.\nuploaded_model_path = \"/kaggle/input/latest-nineteenv3/latest_model1.keras\"\n# For saving, always use the working directory:\nlatest_model_path = os.path.join(save_dir, \"latest_model1.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\n# ✅ Use locally uploaded EfficientNetB1 weights\nweights_file = \"/kaggle/input/efficientnetb1-notop/efficientnetb1_notop.h5\"  # Update path if needed\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB1 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32  # Adjust based on memory availability\ninput_shape = (224, 224, 3)\ninitial_epochs = 3\nfine_tune_epochs = 5\nbase_learning_rate = 0.0005\nfine_tune_learning_rate = 1e-6\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes images to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL CREATION FUNCTION\n# ====================================\ndef create_model(fine_tune=False):\n    base_model = keras.applications.EfficientNetB1(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD OR CREATE MODEL\n# ====================================\n# Try to load the uploaded model first (from /kaggle/input)\nif os.path.exists(uploaded_model_path):\n    model = keras.models.load_model(uploaded_model_path)\n    print(\"✅ Loaded latest model from uploaded file.\")\nelse:\n    model = create_model()\n    print(\"🚀 Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH PER-EPOCH SAVING\n# ====================================\ndef train_model_func(model, epochs, start_epoch=1):\n    # Note: latest_model_path is defined above for saving in /kaggle/working/\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Training epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1)\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)  # Save latest model in /kaggle/working/\n        print(f\"✅ Model saved: {model_epoch_path}\")\n        \n    print(\"✅ Training session complete!\")\n\n# ====================================\n# 8) INITIAL TRAINING PHASE\n# ====================================\nprint(\"\\n=== Training Phase ===\")\n# For example, we resume training starting from epoch 2.\ntrain_model_func(model, epochs=3, start_epoch=4)\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# Before the session ends:\n#   1. Download the file at /kaggle/working/latest_model1.keras.\n# Next time:\n#   2. Upload the downloaded model to your Kaggle dataset (e.g., latest-nineteen).\n#   3. The above code will load the uploaded model from /kaggle/input/latest-nineteen/latest_model1.keras.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"  # Dataset folder\nsave_dir = \"/kaggle/working\"  # Save model & logs\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nuploaded_model_path = \"/kaggle/input/fine-updatev1/latest_model1.keras\"  # Latest model from previous training\nlatest_model_path = os.path.join(save_dir, \"latest_model1.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\n# ✅ Use locally uploaded EfficientNetB1 weights\nweights_file = \"/kaggle/input/efficientnetb1-notop/efficientnetb1_notop.h5\"  # Ensure correct path\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB1 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32  # Can adjust based on GPU memory\ninput_shape = (224, 224, 3)\nfine_tune_epochs = 10  # Start with 10 epochs, can extend if needed\nfine_tune_learning_rate = 5e-6  # Lower LR to prevent overfitting\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # Normalizes images to [-1, 1]\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE FINE-TUNING MODEL\n# ====================================\ndef create_model(fine_tune=True):\n    base_model = keras.applications.EfficientNetB1(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 100  # Unfreeze last 100 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD PREVIOUS MODEL & START FINE-TUNING\n# ====================================\nif os.path.exists(uploaded_model_path):\n    fine_tune_model = create_model(fine_tune=True)\n    fine_tune_model.load_weights(uploaded_model_path)\n    print(\"✅ Loaded latest model from uploaded file. Starting fine-tuning...\")\nelse:\n    print(\"❌ No previous model found. Train the model first before fine-tuning!\")\n    exit()\n\n# ====================================\n# 7) TRAINING FUNCTION WITH CALLBACKS\n# ====================================\ndef train_model_func(model, epochs, start_epoch=1):\n    callbacks = [\n        ModelCheckpoint(latest_model_path, save_best_only=True, verbose=1),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),\n        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n        TensorBoard(log_dir=log_dir)\n    ]\n\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Fine-Tuning Epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1, callbacks=callbacks)\n\n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)  # Save latest model in /kaggle/working/\n        print(f\"✅ Model saved: {model_epoch_path}\")\n\n    print(\"✅ Fine-tuning complete!\")\n\n# ====================================\n# 8) START FINE-TUNING PHASE\n# ====================================\nprint(\"\\n=== Fine-Tuning Phase ===\")\ntrain_model_func(fine_tune_model, epochs=3, start_epoch=8)  # Start from epoch 7\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# Before the session ends:\n#   1. Download the file at /kaggle/working/latest_model1.keras.\n# Next time:\n#   2. Upload the downloaded model to your Kaggle dataset (e.g., latest-nineteen).\n#   3. The above code will load the uploaded model from /kaggle/input/latest-nineteen/latest_model1.keras.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nuploaded_model_path = \"/kaggle/input/fine-updatev2/latest_model1.keras\"\nlatest_model_path = os.path.join(save_dir, \"latest_model1.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\n# ✅ Use locally uploaded EfficientNetB1 weights\nweights_file = \"/kaggle/input/efficientnetb1-notop/efficientnetb1_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB1 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 32  # Adjust based on GPU memory\ninput_shape = (224, 224, 3)\nfine_tune_epochs = 10  # Continue fine-tuning for 10+ more epochs\nfine_tune_learning_rate = 2e-6  # Reduce LR further to improve stability\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE FINE-TUNING MODEL\n# ====================================\ndef create_model(fine_tune=True):\n    base_model = keras.applications.EfficientNetB1(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 150  # Unfreeze last 150 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD PREVIOUS MODEL & START FINE-TUNING\n# ====================================\nif os.path.exists(uploaded_model_path):\n    fine_tune_model = create_model(fine_tune=True)\n    fine_tune_model.load_weights(uploaded_model_path)\n    print(\"✅ Loaded latest model from uploaded file. Starting fine-tuning...\")\nelse:\n    print(\"❌ No previous model found. Train the model first before fine-tuning!\")\n    exit()\n\n# ====================================\n# 7) TRAINING FUNCTION WITH CALLBACKS\n# ====================================\ndef train_model_func(model, epochs, start_epoch=1):\n    callbacks = [\n        ModelCheckpoint(latest_model_path, save_best_only=True, verbose=1),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-7),\n        EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n        TensorBoard(log_dir=log_dir)\n    ]\n\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Fine-Tuning Epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1, callbacks=callbacks)\n\n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)  # Save latest model in /kaggle/working/\n        print(f\"✅ Model saved: {model_epoch_path}\")\n\n    print(\"✅ Fine-tuning complete!\")\n\n# ====================================\n# 8) CONTINUE FINE-TUNING PHASE\n# ====================================\nprint(\"\\n=== Fine-Tuning Phase (Next Steps) ===\")\ntrain_model_func(fine_tune_model, epochs=2, start_epoch=11)  # Continue fine-tuning\n\n# ====================================\n# 9) HOW TO RESUME TRAINING NEXT TIME\n# ====================================\n# Before the session ends:\n#   1. Download the file at /kaggle/working/latest_model1.keras.\n# Next time:\n#   2. Upload the downloaded model to your Kaggle dataset (e.g., latest-nineteen).\n#   3. The above code will load the uploaded model from /kaggle/input/latest-nineteen/latest_model1.keras.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nuploaded_model_path = \"/kaggle/input/fine-updatev11/latest_model1.keras\"\nlatest_model_path = os.path.join(save_dir, \"latest_model1.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\n# ✅ Use locally uploaded EfficientNetB1 weights\nweights_file = \"/kaggle/input/efficientnetb1-notop/efficientnetb1_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB1 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 64  # Try 64 if memory allows\ninput_shape = (224, 224, 3)\nfine_tune_epochs = 10  # Continue fine-tuning for 10+ more epochs\nfine_tune_learning_rate = 5e-6  # Reduce LR further to improve stability\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE FINE-TUNING MODEL\n# ====================================\ndef create_model(fine_tune=True):\n    base_model = keras.applications.EfficientNetB1(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 250  # Unfreeze last 180 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = base_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD PREVIOUS MODEL & START FINE-TUNING\n# ====================================\nif os.path.exists(uploaded_model_path):\n    fine_tune_model = create_model(fine_tune=True)\n    fine_tune_model.load_weights(uploaded_model_path)\n    print(\"✅ Loaded latest model from uploaded file. Starting fine-tuning...\")\nelse:\n    print(\"❌ No previous model found. Train the model first before fine-tuning!\")\n    exit()\n\n# ====================================\n# 7) TRAINING FUNCTION WITH CALLBACKS\n# ====================================\ndef train_model_func(model, epochs, start_epoch=1):\n    callbacks = [\n        ModelCheckpoint(latest_model_path, save_best_only=True, verbose=1),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-7),\n        EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n        TensorBoard(log_dir=log_dir)\n    ]\n\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Fine-Tuning Epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1, callbacks=callbacks)\n\n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)  # Save latest model in /kaggle/working/\n        print(f\"✅ Model saved: {model_epoch_path}\")\n\n    print(\"✅ Fine-tuning complete!\")\n\n# ====================================\n# 8) CONTINUE FINE-TUNING PHASE\n# ====================================\nprint(\"\\n=== Fine-Tuning Phase (Next Steps) ===\")\ntrain_model_func(fine_tune_model, epochs=1, start_epoch=26)  # Continue fine-tuning\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:37:18.784343Z","iopub.execute_input":"2025-03-01T15:37:18.784660Z","iopub.status.idle":"2025-03-01T16:29:09.024656Z","shell.execute_reply.started":"2025-03-01T15:37:18.784632Z","shell.execute_reply":"2025-03-01T16:29:09.023642Z"}},"outputs":[{"name":"stdout","text":"Mixed precision enabled: <DTypePolicy \"mixed_float16\">\nUsing local weights file: /kaggle/input/efficientnetb1-notop/efficientnetb1_notop.h5\nClass indices: {'Azadirachta_indica': 0, 'Baccharis_salicifolia': 1, 'Betula_pendula': 2, 'Camphora_officinarum': 3, 'Ceiba_pentandra': 4, 'Corylus_avellana': 5, 'Delonix_regia': 6, 'Eucalyptus_globulus': 7, 'Ficus_carica': 8, 'Ficus_microcarpa': 9, 'Juglans_regia': 10, 'Leucaena_leucocephala': 11, 'Ligustrum_lucidum': 12, 'Ligustrum_ovalifolium': 13, 'Malus_domestica': 14, 'Mangifera_indica': 15, 'Murraya_paniculata': 16, 'Olea_europaea': 17, 'Paulownia_tomentosa': 18, 'Pinus_halepensis': 19, 'Psidium_guajava': 20, 'Spathodea_campanulata': 21, 'Tecoma_stans': 22, 'Tilia_cordata': 23, 'Ulmus_parvifolia': 24, 'Washingtonia_robusta': 25, 'unknown': 26}\nFound 105631 images belonging to 27 classes.\nFound 26394 images belonging to 27 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 4 variables whereas the saved optimizer has 336 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 2 variables whereas the saved optimizer has 332 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"name":"stdout","text":"✅ Loaded latest model from uploaded file. Starting fine-tuning...\n\n=== Fine-Tuning Phase (Next Steps) ===\n\n🔄 Fine-Tuning Epoch 25/25...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6342 - loss: 1.2162","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: val_loss improved from inf to 1.05171, saving model to /kaggle/working/latest_model1.keras\n\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2894s\u001b[0m 2s/step - accuracy: 0.6342 - loss: 1.2162 - val_accuracy: 0.6838 - val_loss: 1.0517 - learning_rate: 2.0000e-06\n✅ Model saved: /kaggle/working/model_epoch_25.keras\n✅ Fine-tuning complete!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import mixed_precision\nimport datetime\n\n# ====================================\n# 1) Mixed Precision & Basic Setup\n# ====================================\nmixed_precision.set_global_policy('mixed_float16')\nprint(\"Mixed precision enabled:\", mixed_precision.global_policy())\n\n# ====================================\n# 2) CONFIGURATION & PATHS\n# ====================================\nimage_dir = \"/kaggle/input/trees-nineteen-dataset\"\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\n\nmodel_prefix = os.path.join(save_dir, \"model_epoch_\")\nuploaded_model_path = \"/kaggle/input/fine-update/latest_model1.keras\"\nlatest_model_path = os.path.join(save_dir, \"latest_model1.keras\")\nhistory_path = os.path.join(save_dir, \"history.json\")\nclass_indices_path = os.path.join(save_dir, \"class_indices.json\")\nlog_dir = os.path.join(save_dir, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\n# ✅ Use locally uploaded EfficientNetB3 weights\nweights_file = \"/kaggle/input/efficientnetb3/efficientnetb3_notop.h5\"\nif not os.path.exists(weights_file):\n    raise FileNotFoundError(\"Weights file not found! Please upload EfficientNetB3 weights.\")\nelse:\n    print(\"Using local weights file:\", weights_file)\n\nbatch_size = 64  # Can adjust if needed\ninput_shape = (300, 300, 3)  # EfficientNetB3's recommended input size\ninitial_learning_rate = 0.001\nfine_tune_learning_rate = 1e-6  # Lower for fine-tuning stability\nfine_tune_epochs = 10  # Adjust if needed\n\n# ====================================\n# 3) DETECT CLASSES & BUILD CLASS INDICES\n# ====================================\ndetected_classes = sorted(\n    d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))\n)\n\nif os.path.exists(class_indices_path):\n    with open(class_indices_path, \"r\") as f:\n        class_indices = json.load(f)\nelse:\n    class_indices = {name: i for i, name in enumerate(detected_classes)}\n    with open(class_indices_path, \"w\") as f:\n        json.dump(class_indices, f)\nnum_classes = len(class_indices)\nprint(\"Class indices:\", class_indices)\n\n# ====================================\n# 4) SET UP DATA GENERATORS\n# ====================================\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndatagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode=\"nearest\",\n    validation_split=0.2\n)\n\ntrain_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True,\n    classes=list(class_indices.keys())\n)\n\nval_generator = datagen.flow_from_directory(\n    directory=image_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n    classes=list(class_indices.keys())\n)\n\n# ====================================\n# 5) DEFINE MODEL\n# ====================================\ndef create_model(fine_tune=True):\n    base_model = keras.applications.EfficientNetB3(\n        include_top=False,\n        weights=weights_file,\n        input_shape=input_shape\n    )\n\n    if fine_tune:\n        base_model.trainable = True\n        fine_tune_at = len(base_model.layers) - 200  # Unfreeze last 200 layers\n        for layer in base_model.layers[:fine_tune_at]:\n            layer.trainable = False\n        learning_rate = fine_tune_learning_rate\n    else:\n        base_model.trainable = False\n        learning_rate = initial_learning_rate\n\n    model = keras.Sequential([\n        keras.Input(shape=input_shape),\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.BatchNormalization(),\n        layers.Dense(512, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax', dtype='float32')\n    ])\n\n    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# ====================================\n# 6) LOAD MODEL OR TRAIN FROM SCRATCH\n# ====================================\nif os.path.exists(uploaded_model_path):\n    fine_tune_model = create_model(fine_tune=True)\n    fine_tune_model.load_weights(uploaded_model_path)\n    print(\"✅ Loaded latest model from uploaded file. Starting fine-tuning...\")\nelse:\n    fine_tune_model = create_model(fine_tune=False)\n    print(\"🚀 Created new model from scratch.\")\n\n# ====================================\n# 7) TRAINING FUNCTION WITH CALLBACKS\n# ====================================\ndef train_model_func(model, epochs, start_epoch=1):\n    callbacks = [\n        ModelCheckpoint(latest_model_path, save_best_only=True, verbose=1),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-7),\n        EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n        TensorBoard(log_dir=log_dir)\n    ]\n\n    for epoch in range(start_epoch, start_epoch + epochs):\n        print(f\"\\n🔄 Training Epoch {epoch}/{start_epoch + epochs - 1}...\")\n        model.fit(train_generator, validation_data=val_generator, epochs=1, callbacks=callbacks)\n        \n        model_epoch_path = f\"{model_prefix}{epoch}.keras\"\n        model.save(model_epoch_path)\n        model.save(latest_model_path)  # Save latest model in /kaggle/working/\n        print(f\"✅ Model saved: {model_epoch_path}\")\n\n    print(\"✅ Training complete!\")\n\n# ====================================\n# 8) START TRAINING PHASE\n# ====================================\nprint(\"\\n=== Training Phase ===\")\ntrain_model_func(fine_tune_model, epochs=2, start_epoch=1)  # Start training with 3 epochs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T16:46:06.470748Z","iopub.execute_input":"2025-03-01T16:46:06.471060Z","execution_failed":"2025-03-01T19:15:38.699Z"}},"outputs":[{"name":"stdout","text":"Mixed precision enabled: <DTypePolicy \"mixed_float16\">\nUsing local weights file: /kaggle/input/efficientnetb3/efficientnetb3_notop.h5\nClass indices: {'Azadirachta_indica': 0, 'Baccharis_salicifolia': 1, 'Betula_pendula': 2, 'Camphora_officinarum': 3, 'Ceiba_pentandra': 4, 'Corylus_avellana': 5, 'Delonix_regia': 6, 'Eucalyptus_globulus': 7, 'Ficus_carica': 8, 'Ficus_microcarpa': 9, 'Juglans_regia': 10, 'Leucaena_leucocephala': 11, 'Ligustrum_lucidum': 12, 'Ligustrum_ovalifolium': 13, 'Malus_domestica': 14, 'Mangifera_indica': 15, 'Murraya_paniculata': 16, 'Olea_europaea': 17, 'Paulownia_tomentosa': 18, 'Pinus_halepensis': 19, 'Psidium_guajava': 20, 'Spathodea_campanulata': 21, 'Tecoma_stans': 22, 'Tilia_cordata': 23, 'Ulmus_parvifolia': 24, 'Washingtonia_robusta': 25, 'unknown': 26}\nFound 105631 images belonging to 27 classes.\nFound 26394 images belonging to 27 classes.\n🚀 Created new model from scratch.\n\n=== Training Phase ===\n\n🔄 Training Epoch 1/2...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3136 - loss: 2.5763","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Zip the model file (replace with your filename)\nshutil.make_archive('model_checkpoint', 'zip', '/kaggle/working/')\n\n# Now download\nfrom IPython.display import FileLink\nFileLink(r'model_checkpoint.zip')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Create a ZIP file with only the model files\nshutil.make_archive('/kaggle/working/my_models', 'zip', '/kaggle/working', \n                     base_dir=None, verbose=True)\n\n# Generate a download link\nFileLink('/kaggle/working/my_models.zip')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.copy('/kaggle/working/latest_model1.keras', '/kaggle/input/latest_model1.keras')\nshutil.copy('/kaggle/working/model_epoch_2.keras', '/kaggle/input/model_epoch_2.keras')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lh /kaggle/working/\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nos.remove('/kaggle/working/model_checkpoint.zip')  # Delete the large ZIP\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\n\n# Define the zip file name\nzip_filename = \"/kaggle/working/my_models_fixed.zip\"\n\n# Create a zip file and add only the required models\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    zipf.write('/kaggle/working/latest_model1.keras', arcname='latest_model1.keras')\n    zipf.write('/kaggle/working/model_epoch_2.keras', arcname='model_epoch_2.keras')\n\n# Generate a download link\nfrom IPython.display import FileLink\nFileLink(zip_filename)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lh /kaggle/working/\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\n\nzip_filename = \"/kaggle/working/my_models_fixed_nineteen.zip\"\n\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    zipf.write('/kaggle/working/latest_model1.keras', arcname='latest_model1.keras')\n    zipf.write('/kaggle/working/model_epoch_2.keras', arcname='model_epoch_2.keras')\n\nprint(\"✅ Zip file created successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp /kaggle/working/my_models_fixed.zip /kaggle/working/\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install kaggle\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p /kaggle/working/kaggle-dataset\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"meta_data = '''{\n  \"title\": \"My Saved Models\",\n  \"id\": \"your-username/my-saved-models\",\n  \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}'''\n\nwith open(\"/kaggle/working/kaggle-dataset/dataset-metadata.json\", \"w\") as f:\n    f.write(meta_data)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mv /kaggle/working/*.keras /kaggle/working/kaggle-dataset/\n!mv /kaggle/working/*.zip /kaggle/working/kaggle-dataset/\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset/ --public\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p /root/.kaggle\n!mv /kaggle/working/kaggle.json /root/.kaggle/\n!chmod 600 /root/.kaggle/kaggle.json\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p /root/.kaggle\n!cp \"/kaggle/input/kaggle/kaggle (1).json\" /root/.kaggle/kaggle.json\n!chmod 600 /root/.kaggle/kaggle.json\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets list\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset/ --public\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset/ --private\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/my_models.zip /kaggle/working/kaggle-dataset\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset --public\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p ~/.kaggle\n!cp /kaggle/input/kaggle/kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset --public\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle config view\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset --user mohammedelhajsayed --public\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working/kaggle-dataset/\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset --public\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cat /kaggle/working/kaggle-dataset/dataset-metadata.json\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/kaggle-dataset/dataset-metadata.json\n{\n    \"title\": \"MyModelCheckpoints\",\n    \"id\": \"mohammedelhajsayed/mymodelcheckpoints\",\n    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T16:42:41.064159Z","iopub.execute_input":"2025-03-01T16:42:41.064508Z","iopub.status.idle":"2025-03-01T16:42:41.071685Z","shell.execute_reply.started":"2025-03-01T16:42:41.064477Z","shell.execute_reply":"2025-03-01T16:42:41.070881Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/kaggle-dataset/dataset-metadata.json\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset --public\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p /kaggle/working/kaggle-dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T16:42:13.952433Z","iopub.execute_input":"2025-03-01T16:42:13.953333Z","iopub.status.idle":"2025-03-01T16:42:14.171594Z","shell.execute_reply.started":"2025-03-01T16:42:13.953288Z","shell.execute_reply":"2025-03-01T16:42:14.170423Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import shutil\n\nshutil.move(\"/kaggle/working/latest_model1.keras\", \"/kaggle/working/kaggle-dataset/latest_model1.keras\")\nshutil.move(\"/kaggle/working/model_epoch_25.keras\", \"/kaggle/working/kaggle-dataset/model_epoch_25.keras\")\n# shutil.move(\"/kaggle/working/model_epoch_24.keras\", \"/kaggle/working/kaggle-dataset/model_epoch_24.keras\")\n# shutil.move(\"/kaggle/working/model_epoch_10.keras\", \"/kaggle/working/kaggle-dataset/model_epoch_10.keras\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T16:42:15.875410Z","iopub.execute_input":"2025-03-01T16:42:15.875731Z","iopub.status.idle":"2025-03-01T16:42:15.883033Z","shell.execute_reply.started":"2025-03-01T16:42:15.875704Z","shell.execute_reply":"2025-03-01T16:42:15.882374Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/kaggle-dataset/model_epoch_25.keras'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"metadata = '''{\n  \"title\": \"My Model Checkpoints\",\n  \"id\": \"your-kaggle-username/my-model-checkpoints\",\n  \"licenses\": [{\"name\": \"CC0-1.0\"}]\n}'''\n\nwith open(\"/kaggle/working/kaggle-dataset/dataset-metadata.json\", \"w\") as f:\n    f.write(metadata)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T16:42:35.701651Z","iopub.execute_input":"2025-03-01T16:42:35.701968Z","iopub.status.idle":"2025-03-01T16:42:35.706560Z","shell.execute_reply.started":"2025-03-01T16:42:35.701935Z","shell.execute_reply":"2025-03-01T16:42:35.705867Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!mkdir -p /root/.kaggle\n!cp /kaggle/input/kaggle/kaggle.json /root/.kaggle/kaggle.json\n!chmod 600 /root/.kaggle/kaggle.json\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T16:42:37.799282Z","iopub.execute_input":"2025-03-01T16:42:37.799593Z","iopub.status.idle":"2025-03-01T16:42:38.423774Z","shell.execute_reply.started":"2025-03-01T16:42:37.799567Z","shell.execute_reply":"2025-03-01T16:42:38.422167Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/kaggle-dataset --public\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T16:42:43.297753Z","iopub.execute_input":"2025-03-01T16:42:43.298058Z","iopub.status.idle":"2025-03-01T16:42:51.194335Z","shell.execute_reply.started":"2025-03-01T16:42:43.298033Z","shell.execute_reply":"2025-03-01T16:42:51.193236Z"}},"outputs":[{"name":"stdout","text":"Starting upload for file latest_model1.keras\n100%|████████████████████████████████████████| 136M/136M [00:02<00:00, 64.6MB/s]\nUpload successful: latest_model1.keras (136MB)\nStarting upload for file model_epoch_25.keras\n100%|████████████████████████████████████████| 136M/136M [00:02<00:00, 63.1MB/s]\nUpload successful: model_epoch_25.keras (136MB)\nYour public Dataset is being created. Please check progress at https://www.kaggle.com/datasets/mohammedelhajsayed/mymodelcheckpoints\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}